---
layout: post
title: 'Note #3: Laplacian Eigenmaps'
---
### Low effective dimensionality
  * Naturally occurring phenomenon often produces intrinsically low-dimensional data lying in a very high-dimensional space
  * For example, gray-scale images of an object taken with a moving camera
  * In such cases, the dimensionality of the data manifold is determined by the degrees of freedom in the data generation process (eg the camera movement)
  * Even if the manifold is not intrinsic, it might still be practical to visualize the data in lower dimensions 

### Generalised problem of dimensionality reduction
<img src="{{ site.baseurl }}/public/note_4/1.png" width="50">

### Laplacian Eigenmaps preserve local information optimally
__Local information (locality)__: 
<img src="{{ site.baseurl }}/public/note_4/2.png" width="100">

We can encapsulate this property by minimising the following objective function:
<img src="{{ site.baseurl }}/public/note_4/3.png" width="200">

Intuitively if two points are close (connected by an edge with large weight), then minimising the objective function will ensure that their embeddings are close.

Expressing our objective function in a quadratic form:
<img src="{{ site.baseurl }}/public/note_4/4.png" width="250">

Applying constraints to the minimization problem, we can solve it with KKT conditions:
<img src="{{ site.baseurl }}/public/note_4/5.png" width="500">

The first constraint removes an arbitrary scaling factor.
The second constraint eliminates the trivial solution, ie eigenvector = [1,1, â€¦ 1]

For the m-dimensional embedding problem, the solution is provided by retaining the eigenvectors corresponding to the lowest eigenvalues of the generalised eigenvalue problem
<img src="{{ site.baseurl }}/public/note_4/6.png" width="400">

The embedding is given by the _k_ x _m_ matrix where the i<sup>th</sup>row provides the embedding coordinates for the i<sup>th</sup>vertex:
<img src="{{ site.baseurl }}/public/note_4/7.png" width="300">

### Difference from PCA
Both PCA and Laplacian eigenmaps (LE) are used for dimensionality reduction and involve eigen-decomposition. But the similarity stops there. In fact, they are hugely different approaches to dimensionality reduction.

##### Operations: 
* PCA performs eigen-decomposition on the covariance matrix X<sup>T</sup>X, derived from the actual data matrix itself.
* LE uses the graph Laplacian, given by some affinity matrix. Hence, LE is more suitable for graph structured data.

##### Linearity:
* PCA is a linear dimensionality reduction technique. Retained eigenvectors span a subspace that data is then projected onto, thus representing a linear transformation. 
* LE is non-linear. Retained eigenvectors directly provide the embedding vectors.

##### Objective:
* PCA finds the optimal transformations to a subspace with the largest preserved variance. 
* LE preserves local information optimally. 

### Dimensionality reduction and clustering
The local approach to dimensionality reduction yields the same solution as clustering.

If we try to cluster objects into two clusters, a common approach is to minimise the normalised cut, which minimises similarity _between_ clusters, while maximising similarity _within_ clusters. 

It can be shown that if we relax the binary membership to real values, the solution to the relaxed problem is given by the eigenvector corresponding to the smallest non-zero eigenvalue of the normalised graph Laplacian (aka the Fielder's vector).

Spectral clustering proceeds by converting the _soft membership_ to a _hard clustering_ with an auxiliary clustering algorithm (eg k-nearest neighbours). 

In this sense, dimensionality reduction and clustering are two sides of the same coin. Could we measure the quality of the dimension reduction by how well it enhances the cluster properties in the data? 
