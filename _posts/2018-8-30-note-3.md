---
layout: post
title: 'Note #3: Laplacian Eigenmaps'
---
Paper review: 
M. Belkin & P. Niyogi, [Laplacian Eigenmaps for Dimensionality Reduction and Data Representation](http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf), _Neural Computation 2013_
 
### Low effective dimensionality
  * Naturally occurring phenomenon often produces intrinsically low-dimensional data lying in a very high-dimensional space
  * For example, consider a set of gray-scale images of an object taken by a moving camera
  * In such cases, the dimensionality of the data manifold is determined by the degrees of freedom in the data generation process (eg the camera's movement)
  * Even if the manifold is not intrinsic, it might still be practical to visualize the data in lower dimensions 

### Generalised problem of dimensionality reduction
<img src="{{ site.baseurl }}/public/note_4/1.png" width="550">

### Laplacian Eigenmaps preserve local information optimally
__Local information (locality)__: 
<img src="{{ site.baseurl }}/public/note_4/2.png" width="550">

We can encapsulate this property by minimising the following objective function:
<img src="{{ site.baseurl }}/public/note_4/3.png" width="550">

Intuitively if two points are close (connected by an edge with large weight), then minimising the objective function will ensure that their embeddings are close.

Expressing our objective function in a quadratic form:
<img src="{{ site.baseurl }}/public/note_4/4.png" width="550">

Applying constraints, we can solve the resulting minimization problem with KKT conditions:
<img src="{{ site.baseurl }}/public/note_4/5.png" width="550">

The first constraint removes an arbitrary scaling factor in the embedding. <br>
The second constraint eliminates the trivial solution, ie the eigenvector [1,1, â€¦ 1] with eigenvalue 0.

For the m-dimensional embedding problem, the solution is provided by retaining the eigenvectors corresponding to the lowest eigenvalues of the generalised eigenvalue problem:
<img src="{{ site.baseurl }}/public/note_4/6.png" width="550">

The embedding is given by the _k_ x _m_ matrix _Y_, where the i<sup>th</sup> row provides the embedding coordinates for the i<sup>th</sup> vertex:
<img src="{{ site.baseurl }}/public/note_4/7.png" width="550">

### Differences from PCA
Both PCA and Laplacian eigenmaps (LE) are used for dimensionality reduction and involve eigen-decomposition. But the similarity stops there. In fact, they are hugely different approaches to dimensionality reduction.

##### Operations: 
* PCA performs eigen-decomposition on the covariance matrix X<sup>T</sup>X, derived from the actual data matrix itself.
* LE uses the graph Laplacian, given by some affinity matrix. Hence, LE is more suitable for graph structured data.

##### Linearity:
* PCA is a _linear_ dimensionality reduction technique. Retained eigenvectors span a subspace that data is then projected onto, thus representing a linear transformation. 
* LE is _non-linear_. Retained eigenvectors directly provide the embedding vectors.

##### Objective:
* PCA finds the optimal transformations to a subspace with the largest preserved variance. 
* LE preserves local information optimally. 

### Dimensionality reduction and clustering
The local approach to dimensionality reduction imposes a natural clustering of the data.

If we aim to partition objects into two clusters, a common approach is to minimise the normalised cut, which minimises similarity _between_ clusters, while maximising similarity _within_ clusters. 

It can be shown that if we relax the binary membership to real values, the solution to the relaxed problem is given by the eigenvector corresponding to the smallest non-zero eigenvalue of the normalised graph Laplacian (aka the Fielder's vector).

Spectral clustering proceeds by converting the _soft membership_ to a _hard clustering_ with an auxiliary clustering algorithm (eg k-nearest neighbours). 

In this sense, dimensionality reduction and clustering are two sides of the same coin. Could we thus measure the quality of a dimensionality reduction method by how well it enhances the cluster properties in the data? 
