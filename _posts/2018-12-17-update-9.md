---
layout: post
title: 'Update #9'
---
### tSNE on graphs
In the following experiments, we will consider datasets consisting of feature nodes in an existing graph structure in the form (X, A) where:
 * X is a matrix of feature vectors (n x d)
 * A is an adjacency matrix, binary or weighted (n x n)
 
#### A note on two losses
To leverage on the graph information, we shall make use of a composite loss, consisting of a tSNE loss and a graph cut loss. This requires two key assumptions about the data:
<br>    1.   Nodes with _similar features_ should be clustered. (tSNE loss)
<br>    2.   Nodes with _connecting edges_ should be clustered. (Graph cut loss)
 
<img src="{{ site.baseurl }}/public/update_9/losses.png">
 
### Cora citation network
<center>
<img src="{{ site.baseurl }}/public/update_9/cora_summary.png" width="500">
</center>
<img src="{{ site.baseurl }}/public/update_9/cora_graph.png">

### Results
Here, we compare between a graph net and a simple feedforward network. 
<br>
<img src="{{ site.baseurl }}/public/update_9/results.png" width="750">

__Preliminary observations__
 * Minimising the graph cut loss tends to cluster connected nodes together too tightly unless we can enforce a _covariance constraint_.
 * Using _cosine distances_ is very important since the feature vectors are sparse and high dimensional. 
 * For graph nets, the input size at training time must be similar to the input size at test time.
 * Training with randomly sampled subgraphs is important for regularization.

### Potential datasets
<center>
<img src="{{ site.baseurl }}/public/update_9/datasets.png" width="500">
</center>
