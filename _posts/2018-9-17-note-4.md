---
layout: post
title: 'Note #4: An Introduction to tSNE'
---
Paper review: 
L.J.P. Maaten & G. Hinton, [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), _JMLR 2008_

### The intuition behind tSNE
Traditional techniques such as PCA focus on finding a low-dimensional mapping of the data that _maximally preserves variance_. Hence, the squared distance measure favours correctly modeling large pairwise distances. However, large Euclidean distances between two points in high dimensions may not reliably reflect dissimilarities. In the Swiss roll below, the Euclidean distance between the two highlighted points does not accurately reflect how far apart they are on the manifold. 

<p align="center"> 
<img align="center" src="{{ site.baseurl }}/public/note_4/swiss.png" width="310">
</p>
					
On the other hand, _very small_ Euclidean distances accurately reflect points which are similar on the manifold. This is the __focus of tSNE__, which preserves local structure by:
1. Modeling _similar_ datapoints with small pairwise distances in the map
2. Modeling _dissimilar_ datapoints with large (and often amplified) pairwise distances

### Cost function
The two objectives above are modeled into the cost function of tSNE. First off, let's define:
<img src="{{ site.baseurl }}/public/note_4/1.png" width="750">

tSNE aims to find a low-dimensional data representation that _minimizes the mismatch_ between two probability distributions _P_ and _Q_. This is done by minimizing their Kullback-Leibler (KL) divergence, resulting in the following cost function:
<img src="{{ site.baseurl }}/public/note_4/2.png" width="750">

The behaviour of tSNE can easily understood by examining four properties of the cost function. 

#### Property 1: Non-convexity
Optimising the tSNE cost function is trickier than traditional techniques like PCA because the cost function is non-convex. In practice, the cost function is minimised using gradient descent since the gradient has been computed analytically. However, different initialisations can result in quite different embeddings and optimisation is susceptible to local minima. 

#### Property 2: Maps similar points close together
The cost function penalises two cases where a mismatch between _P_ and _Q_ occurs. Because the KL divergence is _asymmetric_, the penalty weight differs in the two cases:
<img src="{{ site.baseurl }}/public/note_4/3.png" width="750">

Hence, we observe that tSNE is focused on modeling similar points in high-dimensional space (high _p<sub>ij</sub>_) close to each other in the low-dimensional map (high _q<sub>ij</sub>_).

#### Property 3: Amplifies distances between dissimilar points
Nevertheless, dissimilar points should also be mapped far apart. In fact, tSNE amplifies these distances because of the distribution chosen to model similarity between datapoints. P consists of the pairwise joint distribution _p<sub>ij</sub>_ which models the similarity between two points in the high-dimensional space. This gives the joint probability that the two points would pick each other as neighbours if a __Gaussian probability__ distribution were centered around each point. 

Likewise, _Q_ models the similarity between two points in the low-dimensional map. However, instead of using a Gaussian, a __Student-t distribution__ is centered around each point. Because the Student-t distribution is heavy-tailed compared to a Gaussian, for a low _p<sub>ij</sub>_ to be mapped to an equally low _q<sub>ij</sub>_, dissimilar points must be pushed further apart in the low-dimensional map.  

For this reason, tSNE visualisations often provide clearer distinctions between dissimilar clusters. It also alleviates the crowding problem that occurs when a dataset of high intrinsic dimensionality (e.g. 10) is _squeezed_ into an embedding space of lower dimensionality (e.g. 2). 

#### Property 4: Computationally expensive
At every gradient descent step, the cost function evaluates the pairwise distances between all points. Hence, tSNE has a computational and memory complexity that is _quadratic_ in the number of datapoints.  This makes standard tSNE infeasible on datasets larger than ~10,000 points. The complexity cost is somewhat alleviated by a variant of tSNE that uses the Barnes-Hut approximation.


