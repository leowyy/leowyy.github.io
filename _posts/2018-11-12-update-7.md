---
layout: post
title: 'Update #7'
---
<html>
<head>
<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css" type="text/css" />
<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js"></script>
</head>
</html>

### TREC Questions Classification
The Text Retrieval Conference (TREC) Questions Classification dataset [[Voorheas, 2000]](https://dl.acm.org/citation.cfm?id=345577) is one of many downstream NLP tasks used to evaluate sentence embeddings. It consists of 5,452 training and 500 test questions organised into one of six categories:
<center>
<img src="{{ site.baseurl }}/public/update_7/trec_samples.png" width="400">
</center>

#### Sentence embeddings
Google's Universal Sentence Encoder [[Cer, 2018]](https://arxiv.org/pdf/1803.11175.pdf) was used to embed the questions into 512-dimensional vectors. The encoder seeks to embed sentences into a vector space whilst _preserving semantic similarities_, which can then be used for downstream NLP tasks.  

Using t-SNE, we can visualise the vectors in 2D. Furthermore, we can make a few observations:
* Questions with similar semantics are clustered together. For example, the cluster in the center left relates to questions about fear. The clustering occurs regardless of the ground truth labels. 
* Clustering by semantics naturally yields some extent of clustering by answer type. 

{% include trec_general.html  %}

#### Why are questions not clustered according to answer type?
When you stop to think, the answer because obvious. At this stage, we have not yet incorporated any knowledge of the classification task and ground truth labels. The vectors purely encode for the semantic information in the sentence, thus t-SNE yields clusters of similar semantics.

#### What makes two things similar?
One of the main goals of visualisation or clustering is to group similar items and separate dissimilar items. But the concept of similarity is not fixed; _it depends largely on context_. 

For instance, consider three different movies:
1. Terminator
2. Die Hard
3. Titanic

Terminator and Die Hard are clearly similar because they are both _action movies_. But in another context, Terminator is also similar to Titanic - they are both _directed_ by James Cameron. _Similarity depends on context_. 

#### Training a question classifier on TREC
The same phenomenon occurs in NLP as well. Consider now three questions:
1. Why are haunted houses popular?
2. What is a fear of thunder? (Entity answer type)
3. What animal has the best hearing? (Entity answer type)

In the graph above, questions 1&2 are clustered together because they have the same semantics. But for the sake of our classification task, we want 2&3 to be clustered together. _What we need is embeddings which are task-specific_.

<center>
<img src="{{ site.baseurl }}/public/update_7/mlp.png" width="700">
</center>
For this, a multi-layer perceptron (MLP) with a single hidden layer of size 50 was trained following [[Perone, 2018]](https://arxiv.org/pdf/1806.06259.pdf). The MLP yields a classification accuracy of 90% on the test set. 

Now, we perform t-SNE on the 50-dimensional features extracted from the hidden layer.

{% include trec.html  %}

As expected, we can now observe clusters according to the answer type. An important insight comes forth:
> Supervised training yields task-specific representations.

This phenomenon is well understood in the field of computer vision [[Yosinki, 2014]](https://arxiv.org/abs/1411.1792). In deep CNNs, the first layers act as Gabor filters or edge detectors, while the later layers detect higher-level features such as texture. The transition of feature vectors in the network from general to task-specific allows for transfer learning. 

When we perform data visualisation or clustering, we should remind ourselves of the context. 
* When do we say that two things are similar? 
* What do the feature vectors represent? 
* Are the vector representations consistent with the chosen definition of similarity?
