---
layout: post
title: 'Update #7'
---
<html>
<head>
<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css" type="text/css" />
<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js"></script>
</head>
</html>

### TREC Questions Classification
The Text Retrieval Conference (TREC) Questions Classification dataset is one of many downstream NLP tasks used to evaluate sentence embeddings. It consists of 5,452 training and 500 test questions organised into one of six categories:
<center>
<img src="{{ site.baseurl }}/public/update_7/trec_samples.png" width="500">
</center>

#### __Sentence embeddings__
Google's Universal Sentence Encoder [[Cer, 2018]](https://arxiv.org/pdf/1803.11175.pdf) was used to embed the questions into 512-dimensional vectors. The encoder seeks to embed sentences into a vector space whilst preserving semantic similarities, which can then be used for downstream NLP tasks.  

Using t-SNE, we can visualise the vectors in 2D. Furthermore, we can make a few observations:
* Questions with similar semantics are clustered together. For example, the cluster at the bottom left relates to questions about fear. The clustering occurs regardless of the ground truth labels. 
* Clustering by semantics naturally yields some extent of clustering by answer type. 

{% include trec_general.html  %}

#### __Why are questions not clustered according to answer type?__
When you stop to think, the answer because obvious. At this stage, we have not yet incorporated any knowledge of the classification task and ground truth labels. The vectors purely encode for the semantic information in the sentence, thus t-SNE yields clusters of similar semantics.

#### __What makes two things similar?__
One of the main goals of visualisation or clustering is to group similar items and separate dissimilar items. But the concept of similarity is not fixed; it depends largely on context. 

For instance, consider three different movies:
1. Terminator
2. Die Hard
3. Titanic

Terminator and Die Hard are clearly similar because they are both action movies. But in another context, Terminator is also similar to Titanic - they are both directed by James Cameron. Similarity depends on context. 

#### __Training a question classifier on TREC__
The same phenomenon occurs in NLP as well. Consider now three questions:
1. Why are haunted houses popular?
2. What is a fear of thunder? (Entity answer type)
3. What animal has the best hearing? (Entity answer type)

In the graph above, questions 1&2 are clustered together because they have the same semantics. But for the sake of our classification task, we want 2&3 to be clustered together. What we need is to obtain task-related embeddings.

For this, a multi-layer perceptron (MLP) with a single hidden layer of size 50 was trained following [[Perone, 2018]](https://arxiv.org/pdf/1806.06259.pdf). The MLP yields a classification accuracy of 90% on the test set. 

Now, we perform t-SNE on the 50-dimensional features extracted from the hidden layer.

{% include trec.html  %}

As expected, we can now observe clusters according to the answer type. An important insight comes forth:
Supervised training yields task-specific representations.

This phenomenon is well understood in the field of computer vision [[Yosinki, 2014]](https://arxiv.org/abs/1411.1792). In deep CNNs, the first layers act as Gabor filters or edge detectors, while the later layers detect higher-level features such as texture. The transition of feature vectors in the network from general to task-specific allows for transfer learning. 

When we perform data visualisation or clustering, we should remind ourselves of the context. When do we say that two things are similar? What do the feature vectors represent? Are the vector representations consistent with the chosen definition of similarity?
