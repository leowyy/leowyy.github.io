---
layout: post
title: 'Update #3'
---
### Experiment set-up: 
In the following experiments, a reference test set was prepared which consists of 100 separate data sets of sizes in the range [200,500].
Training for spectral used: 10,000 training examples for 100 iterations, in mini-batches of [200,500]

### Experiment 1: Evaluation measures 
Trustworthiness [[Venna, 2001]](https://link.springer.com/chapter/10.1007/3-540-44668-0_68) measures the amount of local structure that is preserved, ranging between [0,1]. A score of 1 roughly indicates that all the nearest neighbours in the high-dimensional space are preserved in the low-dimensional map. 

1-NN generalisation error [[Maaten, 2009]](http://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S18/DimRed2.pdf) measures the 10-fold cross-validation error of a one nearest-neighbour classifier trained on the low-dimensional data representations of each test data set.

<img src="{{ site.baseurl }}/public/update_3/1.png">

### Experiment 2: Loss function used during training
<img src="{{ site.baseurl }}/public/update_3/2.png">

Training on the pairwise loss leads to significant improvements. The L2 loss is not able to reflect the invariance under rotations and reflection when using spectral embeddings.

### Experiment 3: Conv net vs Graph net
<img src="{{ site.baseurl }}/public/update_3/3.png">

A simple conv net was used with the following architecture: <br>
  * Conv 1 (5x5 filters) x50 - ReLU - BatchNorm - MaxPool (3x3, stride 2) - <br>
  * Conv 2 (5x5 filters) x50 - ReLU - BatchNorm - MaxPool (3x3, stride 2) - <br>
  * Fully connected layer

#### Comparison of the number of parameters
  * Conv net: 68,952 
  * Graph net: 298,102 

### Experiment 4: Training on Spectral vs t-SNE
Training for t-SNE used: 20,000 training examples for 200 iterations, in mini-batches of [200,500]

<img src="{{ site.baseurl }}/public/update_3/4.png">

### Experiment 5: Visualising outputs 
<img src="{{ site.baseurl }}/public/update_3/four_plots.png">

#### Observations
  * The output of the graph net is _deterministic_; clusters of a certain type always map to the same region in the embedding space.
  * The global structure of spectral embeddings is the same in all four cases (subject to rotations and reflections). However, the global structure of t-SNE embeddings varies widely. This is the expected behaviour of t-SNE as the sizes, and the absolute/relative positions of t-SNE clusters may not carry any intrinsic meaning [[Waatenberg, 2016]](https://distill.pub/2016/misread-tsne/).
