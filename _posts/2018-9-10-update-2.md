---
layout: post
title: 'Update #2'
---
### Done
  * Read X. Bresson & T. Laurent, [An Experimental Study of Neural Networks for Variable Graphs](https://openreview.net/pdf?id=SJexcZc8G), _ICLR 2018_
  * Read M. Belkin & P. Niyogi, [Laplacian Eigenmaps for Dimensionality Reduction and Data Representation](http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf), _Neural Computation 2003_

### Experiment 1
In the following experiment, I consider the graph CNN used for semi-supervised clustering.

The graph CNN is given a graph input consisting of a set of nodes, and passes it through 10 hidden layers, each with an embedding size of 50 dimensions. 

__Q__: Can we visualise the embedding vectors at each hidden layer?

__A__: Extract the embedding vectors and plot them with non-linear dimensionality reduction!

Here are the results I obtained: 
![Visualise hidden layers]({{ site.baseurl }}/public/embeddings.png)

### Experiment 2
__Q__: Can we use a graph CNN to learn non-linear dimensionality reduction?

__A__: Maybe?

Pseudocode for training the net:
```python
# Set up net architecture and parameters
net = GraphConvNet(net_parameters)

for iteration in range(max_iters):
  # Obtain subset of mnist data
  x_train = get_data_subset()

  # Compute affinity matrix
  affinity = affinity_matrix(x_train)

  # Compute training labels from Laplacian eigenmaps
  y_train = spectral_embedding(x_train, affinity)

  # Forward propagation
  y_pred = net.forward(x_train, affinity)

  # Compute L2 loss
  loss = net.loss(y_pred, y_train)

  # Backprop
  loss.backward()
```

### Next steps
  * Read L. Maaten & G. Hinton, [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), _JMLR 2008_
