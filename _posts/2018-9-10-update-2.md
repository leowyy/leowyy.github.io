---
layout: post
title: 'Update #2'
---
### Done
  * Read X. Bresson & T. Laurent, [An Experimental Study of Neural Networks for Variable Graphs](https://openreview.net/pdf?id=SJexcZc8G), _ICLR 2018_
  * Read M. Belkin & P. Niyogi, [Laplacian Eigenmaps for Dimensionality Reduction and Data Representation](http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf), _Neural Computation 2003_

### Experiment 1
In the following experiment, I consider the graph CNN used for semi-supervised clustering.

The graph CNN is given a graph input consisting of a set of nodes, and passes it through 10 hidden layers, each with an embedding size of 50 dimensions. 

__Q__: Can we visualise the embedding vectors at each hidden layer?

__A__: Extract the embedding vectors and plot them with non-linear dimensionality reduction!

Here are the results I obtained: 
![Visualise hidden layers]({{ site.baseurl }}/public/embeddings.png)

### Experiment 2
__Q__: Can we use a graph CNN to learn non-linear dimensionality reduction?

__A__: Maybe?

### Next steps
  * Read L. Maaten & G. Hinton, [Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), _JMLR 2008_
